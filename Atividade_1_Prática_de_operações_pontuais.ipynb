{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPUuzeIK8oetvfK/gfmS0sm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jp-prud/PDI/blob/main/Atividade_1_Pr%C3%A1tica_de_opera%C3%A7%C3%B5es_pontuais.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Prática com Operações Pontuais e OpenCV\n",
        "\n",
        "Conversão de imagem RGB em imagem Grayscale"
      ],
      "metadata": {
        "id": "q_2qAHvxocSI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f_S_cq46nv0R"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "img = cv2.imread('t1.jpg')\n",
        "\n",
        "B, G, R = cv2.split(img)\n",
        "img_grayscale_basic_cv2 = cv2.addWeighted(B, 1/3.0, G, 1/3.0, 0)\n",
        "img_grayscale_basic_cv2 = cv2.addWeighted(img_grayscale_basic_cv2, 1.0, R, 1/3.0, 0)\n",
        "\n",
        "cv2.imshow('Imagem em Tons de Cinza', img_grayscale_basic_cv2)\n",
        "cv2.waitKey(0)\n",
        "cv2.destroyAllWindows()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "[Imagem em Tons de Cinza](https://imgur.com/refUWK0)"
      ],
      "metadata": {
        "id": "7z6jRbIc7Mlb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Conversão ponderada"
      ],
      "metadata": {
        "id": "j6uJZHY3paus"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "weight_b = 0.3\n",
        "weight_g = 0.59\n",
        "weight_r = 0.11\n",
        "\n",
        "img_grayscale_pondered_np = (weight_b * img[:, :, 0] +\n",
        "                              weight_g * img[:, :, 1] +\n",
        "                              weight_r * img[:, :, 2])\n",
        "\n",
        "B, G, R = cv2.split(img)\n",
        "img_grayscale_pondered_cv2 = (weight_b * B + weight_g * G + weight_r * R)\n",
        "img_grayscale_pondered_cv2 = cv2.convertScaleAbs(img_grayscale_pondered_cv2)\n",
        "img_grayscale_pondered = np.array(img_grayscale_pondered_cv2, dtype=np.uint8)\n",
        "\n",
        "cv2.imshow('Imagem com Média Ponderada', img_grayscale_pondered)\n",
        "cv2.waitKey(0)\n",
        "cv2.destroyAllWindows()"
      ],
      "metadata": {
        "id": "8brVFPlppewT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "[Conversão ponderada](https://imgur.com/RdY1e4T)"
      ],
      "metadata": {
        "id": "Ze4Jp1vb-fb-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Histograma"
      ],
      "metadata": {
        "id": "1zV3KH1pptHW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "intenses = np.linspace(0, 255, 256)\n",
        "histogram, _ = np.histogram(img_grayscale_basic_cv2.ravel(), bins=256, range=(0, 256))\n",
        "plt.bar(intenses, histogram, width=1.0, color='gray')\n",
        "\n",
        "plt.xlabel('Intensidade de Pixel')\n",
        "plt.ylabel('Frequência')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "TcyluELdpxlX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "[Histograma](https://imgur.com/Aqv4EwW)"
      ],
      "metadata": {
        "id": "lmrIzJdQ_cnM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Histograma normalizado"
      ],
      "metadata": {
        "id": "OTU_DY_6qLe_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "intensidades = np.linspace(0, 255, 256)\n",
        "\n",
        "histogram, _ = np.histogram(img.ravel(), bins=256, range=(0, 256))\n",
        "histogram_normalized = histogram / float(np.sum(histogram))\n",
        "\n",
        "plt.plot(intensidades, histogram_normalized, color='gray')\n",
        "\n",
        "plt.xlabel('Intensidade de Pixel')\n",
        "plt.ylabel('Frequência Normalizada')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "lpa8MAQRqRi3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "[Histograma normalizado](https://imgur.com/6n8TF6h)\n",
        "\n"
      ],
      "metadata": {
        "id": "xr2jW-QA_uzD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Transformação negativa"
      ],
      "metadata": {
        "id": "R_nggSu1qZ-V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "img_in = np.uint8(img)\n",
        "img_negative_np = 255 - img_in\n",
        "cv2.imshow('Entrada', img_in)\n",
        "\n",
        "cv2.imshow('Saída', img_negative_np)\n",
        "cv2.waitKey(0)\n",
        "cv2.destroyAllWindows()"
      ],
      "metadata": {
        "id": "6hwag1oDqi6O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Transforma uma imagem em negativa invertendo os valores dos pixels, onde cada pixel da imagem de entrada 'img' é subtraído de 255 para obter a imagem negativa. [Transformação negativa](https://imgur.com/8IHXh6g)"
      ],
      "metadata": {
        "id": "g6J8EtdJASQS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Transformação logaritimica"
      ],
      "metadata": {
        "id": "iMx1vNocqq4G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "img_in = np.uint8(img)\n",
        "\n",
        "c = 0.1\n",
        "img_out = c * np.log1p(img_in)\n",
        "img_out = np.uint8(img_out)\n",
        "\n",
        "cv2.imshow('Entrada', img_in)\n",
        "\n",
        "cv2.imshow('Saída', img_out)\n",
        "cv2.waitKey(0)\n",
        "cv2.destroyAllWindows()"
      ],
      "metadata": {
        "id": "G8NMKmf0qug5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "aplica uma transformação logarítmica suave na imagem de entrada 'img_in', resultando em um novo array 'img_out'. [Transformação logaritimica](https://imgur.com/4onSQFv)"
      ],
      "metadata": {
        "id": "mWa2chyJCuLc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Transformação gamma"
      ],
      "metadata": {
        "id": "zQelwE2fqxNT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "img_in = np.uint8(img)\n",
        "\n",
        "c = 1.0\n",
        "gamma = 0.8\n",
        "\n",
        "img_out = c * (img_in ** gamma)\n",
        "\n",
        "img_out = np.uint8(img_out)\n",
        "\n",
        "cv2.imshow('Entrada', img_in)\n",
        "\n",
        "cv2.imshow('Saída', img_out)\n",
        "cv2.waitKey(0)\n",
        "cv2.destroyAllWindows()"
      ],
      "metadata": {
        "id": "k0jsitQOq07f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " Cada pixel da imagem de saída 'img_out' é calculado elevando o pixel correspondente da imagem de entrada 'img' à potência de 'gamma' e multiplicando o resultado por 'c'. [Transformação gamma](https://imgur.com/COlNSUy)"
      ],
      "metadata": {
        "id": "2TiqTFGsDLbb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Transformação de alargamento de contraste"
      ],
      "metadata": {
        "id": "sDOMqiptq-aw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "min_limit = 50\n",
        "max_limit = 200\n",
        "\n",
        "height, width = img.shape\n",
        "img_out = img.copy()\n",
        "\n",
        "for y in range(height):\n",
        "    for x in range(width):\n",
        "        pixel_value = img[y, x]\n",
        "        if pixel_value < min_limit:\n",
        "            img_out[y, x] = 0\n",
        "        elif pixel_value > max_limit:\n",
        "            img_out[y, x] = 255\n",
        "        else:\n",
        "            img_out[y, x] = int((pixel_value - min_limit) * (255 / (max_limit - min_limit)))\n",
        "\n",
        "cv2.imshow('Entrada', img)\n",
        "cv2.imshow('Saída', img_out)\n",
        "cv2.waitKey(0)\n",
        "cv2.destroyAllWindows()"
      ],
      "metadata": {
        "id": "4GGQbpd0rLPT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Percorre cada pixel na imagem original e aplica a transformação de alargamento de contraste. Para cada pixel, o código verifica se o valor de intensidade está abaixo de min_limit e define o valor de saída para 0 se for o caso. Se o valor de intensidade estiver acima de max_limit, define o valor de saída como 255. Se o valor de intensidade estiver entre min_limit e max_limit, ele é ajustado para um novo valor no intervalo de 0 a 255 com base na escala. [Alargamento de contraste](https://imgur.com/HlaQ4EA)"
      ],
      "metadata": {
        "id": "WSZdfBRaJuxi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Transformação de contraste e brilho"
      ],
      "metadata": {
        "id": "B57Tq7bmtJ_f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "a = 1.0\n",
        "b = 50\n",
        "\n",
        "img_out = cv2.convertScaleAbs(img, alpha=a, beta=b)\n",
        "\n",
        "cv2.imshow('Entrada', img)\n",
        "\n",
        "cv2.imshow('Saída', img)\n",
        "cv2.waitKey(0)\n",
        "cv2.destroyAllWindows()"
      ],
      "metadata": {
        "id": "8kU41a79tJug"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " Aplica a transformação linear na imagem de entrada img. A intensidade de cada pixel é multiplicada por a e, em seguida, é adicionado o valor b.\n",
        "\n",
        "[Tranformação de contrate e brilho](https://imgur.com/L0ne9k0)"
      ],
      "metadata": {
        "id": "ySWJI7usKXt9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Transformação de limiar"
      ],
      "metadata": {
        "id": "zTKLr8pMyw-f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "img_in = np.uint8(img)\n",
        "\n",
        "limiar = 128\n",
        "\n",
        "img_out = np.zeros_like(img)\n",
        "\n",
        "for i in range(3):\n",
        "    img_out[:, :, i] = np.where(img[:, :, i] > limiar, 255, 0)\n",
        "\n",
        "cv2.imshow('Entrada', img)\n",
        "\n",
        "cv2.imshow('Saída', img_out)\n",
        "cv2.waitKey(0)\n",
        "cv2.destroyAllWindows()"
      ],
      "metadata": {
        "id": "RyHGGB4gywjI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pixels com valores acima de 128 são definidos como 255 (branco), e pixels com valores iguais ou abaixo de 128 são definidos como 0 (preto). [Transformação limiar](https://imgur.com/GqX94Lw)"
      ],
      "metadata": {
        "id": "tV94x7XtM4K4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " Transformação de limiar com fatiamento de plano"
      ],
      "metadata": {
        "id": "5MG_v2eazc4s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "B, G, R = cv2.split(img)\n",
        "\n",
        "img_gray = cv2.addWeighted(B, 1/3.0, G, 1/3.0, 0)\n",
        "img_gray = cv2.addWeighted(img_gray, 1.0, R, 1/3.0, 0)\n",
        "\n",
        "limiar = 128\n",
        "\n",
        "height, width = img_gray.shape\n",
        "img_thresh = np.zeros((height, width), dtype=np.uint8)\n",
        "for y in range(height):\n",
        "    for x in range(width):\n",
        "        pixel = img[y, x]\n",
        "        gray_value = (int(pixel[0]) + int(pixel[1]) + int(pixel[2])) // 3\n",
        "        img_thresh[y, x] = 255 if gray_value > limiar else 0\n",
        "\n",
        "cv2.imshow('Entrada', img_gray)\n",
        "cv2.imshow('Saída', img_thresh)\n",
        "cv2.waitKey(0)\n",
        "cv2.destroyAllWindows()"
      ],
      "metadata": {
        "id": "HqxHVyfOzF3H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "[Limiar de fatiamento de plano](https://imgur.com/CmxyYS7)"
      ],
      "metadata": {
        "id": "ICdFMMO83gXT"
      }
    }
  ]
}